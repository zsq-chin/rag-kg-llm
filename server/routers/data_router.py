import os
import asyncio
import traceback
import fastapi
from distutils.file_util import copy_file
import subprocess
from email.quoprimime import unquote

from pydantic import BaseModel
from fastapi import Response
from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Body, Form, Query
from fastapi.responses import FileResponse
from urllib.parse import unquote, quote

from starlette.responses import StreamingResponse

from src.utils import logger, hashstr
from src import executor, retriever, config, knowledge_base, graph_base
from server.utils.auth_middleware import get_admin_user
from server.models.user_model import User
from typing import List, Optional
from fastapi.responses import JSONResponse
from pathlib import Path
import time
import requests
import pandas as pd
from pathlib import Path as PathlibPath
import shutil
data = APIRouter(prefix="/data")
UPLOAD_DIR = Path("D:\shanhai\sage-master\sage-master\saves\data\graphragfile")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

@data.get("/")
async def get_databases(current_user: User = Depends(get_admin_user)):
    try:
        database = knowledge_base.get_databases()
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“åˆ—è¡¨å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"è·å–æ•°æ®åº“åˆ—è¡¨å¤±è´¥ {e}", "databases": []}
    return database

@data.post("/")
async def create_database(
    database_name: str = Body(...),
    description: str = Body(...),
    dimension: int | None = Body(None),
    current_user: User = Depends(get_admin_user)
):
    logger.debug(f"Create database {database_name}")
    try:
        existing_dbs_dict = knowledge_base.get_databases()  
        db_list = existing_dbs_dict.get("databases", [])  
        if any(db.get("name") == database_name for db in db_list):
            raise HTTPException(
                status_code=400,
                detail=f"æ•°æ®åº“å '{database_name}' å·²å­˜åœ¨"
            )
        database_info = knowledge_base.create_database(
            database_name,
            description,
            dimension=dimension
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"åˆ›å»ºæ•°æ®åº“å¤±è´¥ {e}", "status": "failed"}
    return database_info


def convert_to_graph_format(input_csv_path: PathlibPath, output_csv_path: PathlibPath) -> dict:
    """
    å°†çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è½¬æ¢ä¸ºå›¾æ•°æ®åº“ä¸Šä¼ æ ¼å¼

    Args:
        input_csv_path: è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„
        output_csv_path: è¾“å‡ºçš„CSVæ–‡ä»¶è·¯å¾„

    Returns:
        dict: è½¬æ¢ç»“æœä¿¡æ¯
    """
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not input_csv_path.exists():
            return {"status": "error", "detail": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_csv_path}"}

        # è¯»å–CSVæ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {input_csv_path}")
        df = pd.read_csv(input_csv_path)

        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['source', 'target', 'description']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            return {
                "status": "error",
                "detail": f"CSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}",
                "available_columns": list(df.columns)
            }

        # æå–éœ€è¦çš„åˆ—å¹¶é‡å‘½å
        graph_df = df[['source', 'description', 'target']].copy()
        graph_df.columns = ['h', 'r', 't']  # é‡å‘½åä¸ºå›¾æ•°æ®åº“è¦æ±‚çš„æ ¼å¼

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_csv_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºæ–°çš„CSVæ–‡ä»¶
        graph_df.to_csv(output_csv_path, index=False, encoding='utf-8')

        print(f"âœ… å·²æˆåŠŸè½¬æ¢æ–‡ä»¶æ ¼å¼")
        print(f"ğŸ“Š è½¬æ¢ç»Ÿè®¡: {len(graph_df)} æ¡å…³ç³»")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_csv_path}")

        return {
            "status": "success",
            "detail": "æ–‡ä»¶æ ¼å¼è½¬æ¢æˆåŠŸ",
            "input_file": str(input_csv_path),
            "output_file": str(output_csv_path),
            "relationship_count": len(graph_df),
            "sample_data": graph_df.head(3).to_dict('records')  # è¿”å›å‰3æ¡æ•°æ®ä½œä¸ºç¤ºä¾‹
        }

    except Exception as e:
        return {"status": "error", "detail": f"æ–‡ä»¶æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}"}
@data.delete("/")
async def delete_database(db_id, current_user: User = Depends(get_admin_user)):
    logger.debug(f"Delete database {db_id}")
    knowledge_base.delete_database(db_id)
    return {"message": "åˆ é™¤æˆåŠŸ"}

@data.post("/query-test")
async def query_test(query: str = Body(...), meta: dict = Body(...), current_user: User = Depends(get_admin_user)):
    logger.debug(f"Query test in {meta}: {query}")
    result = retriever.query_knowledgebase(query, history=None, refs={"meta": meta})
    return result

@data.post("/file-to-chunk")
async def file_to_chunk(db_id: str = Body(...), files: list[str] = Body(...), params: dict = Body(...), current_user: User = Depends(get_admin_user)):
    logger.debug(f"File to chunk for db_id {db_id}: {files} {params=}")
    try:
        processed_files = await knowledge_base.save_files_for_pending_indexing(db_id, files, params)
        return {"message": "Files processed and pending indexing", "files": processed_files, "status": "success"}
    except Exception as e:
        logger.error(f"Failed to process files for pending indexing: {e}, {traceback.format_exc()}")
        return {"message": f"Failed to process files for pending indexing: {e}", "status": "failed"}

@data.post("/url-to-chunk")
async def url_to_chunk(db_id: str = Body(...), urls: list[str] = Body(...), params: dict = Body(...), current_user: User = Depends(get_admin_user)):
    logger.debug(f"Url to chunk for db_id {db_id}: {urls} {params=}")
    try:
        processed_urls = await knowledge_base.save_urls_for_pending_indexing(db_id, urls, params)
        return {"message": "URLs processed and pending indexing", "urls": processed_urls, "status": "success"}
    except Exception as e:
        logger.error(f"Failed to process URLs for pending indexing: {e}, {traceback.format_exc()}")
        return {"message": f"Failed to process URLs for pending indexing: {e}", "status": "failed"}

@data.post("/add-by-file")
async def create_document_by_file(db_id: str = Body(...), files: list[str] = Body(...), current_user: User = Depends(get_admin_user)):
    raise ValueError("This method is deprecated. Use /file-to-chunk and /index-file instead.")

@data.post("/add-by-chunks")
async def add_by_chunks(db_id: str = Body(...), file_chunks: dict = Body(...), current_user: User = Depends(get_admin_user)):
    raise ValueError("This method is deprecated. Use /file-to-chunk and /index-file instead.")

@data.post("/index-file")
async def index_file(db_id: str = Body(...), file_id: str = Body(...), current_user: User = Depends(get_admin_user)):
    logger.debug(f"Indexing file_id {file_id} in db_id {db_id}")
    try:
        result = await knowledge_base.trigger_file_indexing(db_id, file_id)
        return {"message": f"File {file_id} indexing initiated", "details": result, "status": "success"}
    except Exception as e:
        logger.error(f"Failed to index file {file_id}: {e}, {traceback.format_exc()}")
        return {"message": f"Failed to index file {file_id}: {e}", "status": "failed"}

@data.get("/info")
async def get_database_info(db_id: str, current_user: User = Depends(get_admin_user)):
    # logger.debug(f"Get database {db_id} info")
    database = knowledge_base.get_database_info(db_id)
    if database is None:
        raise HTTPException(status_code=404, detail="Database not found")
    return database

@data.delete("/document")
async def delete_document(db_id: str = Body(...), file_id: str = Body(...), current_user: User = Depends(get_admin_user)):
    logger.debug(f"DELETE document {file_id} info in {db_id}")
    knowledge_base.delete_file(db_id, file_id)
    return {"message": "åˆ é™¤æˆåŠŸ"}

@data.get("/document")
async def get_document_info(db_id: str, file_id: str, current_user: User = Depends(get_admin_user)):
    logger.debug(f"GET document {file_id} info in {db_id}")

    try:
        info = knowledge_base.get_file_info(db_id, file_id)
    except Exception as e:
        logger.error(f"Failed to get file info, {e}, {db_id=}, {file_id=}, {traceback.format_exc()}")
        info = {"message": "Failed to get file info", "status": "failed"}

    return info

@data.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    db_id: str | None = Query(None),
    current_user: User = Depends(get_admin_user)
):
    if not file.filename:
        raise HTTPException(status_code=400, detail="No selected file")

    # æ ¹æ®db_idè·å–ä¸Šä¼ è·¯å¾„ï¼Œå¦‚æœdb_idä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if db_id:
        upload_dir = knowledge_base.get_db_upload_path(db_id)
    else:
        upload_dir = os.path.join(config.save_dir, "data", "uploads")

    basename, ext = os.path.splitext(file.filename)
    filename = f"{basename}_{hashstr(basename, 4, with_salt=True)}{ext}".lower()
    file_path = os.path.join(upload_dir, filename)
    os.makedirs(upload_dir, exist_ok=True)

    with open(file_path, "wb") as buffer:
        buffer.write(await file.read())

    return {"message": "File successfully uploaded", "file_path": file_path, "db_id": db_id}

@data.get("/graph")
async def get_graph_info(current_user: User = Depends(get_admin_user)):
    graph_info = graph_base.get_graph_info()
    if graph_info is None:
        raise HTTPException(status_code=400, detail="å›¾æ•°æ®åº“è·å–å‡ºé”™")
    return graph_info

@data.post("/graph/index-nodes")
async def index_nodes(data: dict = Body(default={}), current_user: User = Depends(get_admin_user)):
    if not graph_base.is_running():
        raise HTTPException(status_code=400, detail="å›¾æ•°æ®åº“æœªå¯åŠ¨")

    # è·å–å‚æ•°æˆ–ä½¿ç”¨é»˜è®¤å€¼
    kgdb_name = data.get('kgdb_name', 'neo4j')

    # è°ƒç”¨GraphDatabaseçš„add_embedding_to_nodesæ–¹æ³•
    count = graph_base.add_embedding_to_nodes(kgdb_name=kgdb_name)

    return {"status": "success", "message": f"å·²æˆåŠŸä¸º{count}ä¸ªèŠ‚ç‚¹æ·»åŠ åµŒå…¥å‘é‡", "indexed_count": count}

@data.get("/graph/node")
async def get_graph_node(entity_name: str, current_user: User = Depends(get_admin_user)):
    result = graph_base.query_node(entity_name=entity_name)
    return {"result": graph_base.format_query_result_to_graph(result), "message": "success"}

@data.get("/graph/nodes")
async def get_graph_nodes(kgdb_name: str, num: int, current_user: User = Depends(get_admin_user)):
    if not config.enable_knowledge_graph:
        raise HTTPException(status_code=400, detail="Knowledge graph is not enabled")

    logger.debug(f"Get graph nodes in {kgdb_name} with {num} nodes")
    result = graph_base.get_sample_nodes(kgdb_name, num)
    return {"result": graph_base.format_general_results(result), "message": "success"}

@data.post("/graph/add-by-jsonl")
async def add_graph_entity(file_path: str = Body(...), kgdb_name: str | None = Body(None), current_user: User = Depends(get_admin_user)):
    if not config.enable_knowledge_graph:
        return {"message": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨", "status": "failed"}

    if not file_path.endswith('.csv'):
        return {"message": "æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸Šä¼  csv æ–‡ä»¶", "status": "failed"}

    try:
        await graph_base.jsonl_file_add_entity(file_path, kgdb_name)
        return {"message": "å®ä½“æ·»åŠ æˆåŠŸ", "status": "success"}
    except Exception as e:
        logger.error(f"æ·»åŠ å®ä½“å¤±è´¥: {e}, {traceback.format_exc()}")
        return {"message": f"æ·»åŠ å®ä½“å¤±è´¥: {e}", "status": "failed"}
#å¤„ç†æ–‡ä»¶
class FileHandleRequest(BaseModel):
    file_path: str
@data.post("/graph/handle")
async def graphfile_handle(request: FileHandleRequest):
    file_path = request.file_path
    '''é¦–å…ˆè¿›è¡Œæ–‡ä»¶å¤„ç†'''
    EXTERNAL_API_URL = "http://host.docker.internal:8000/api/v1/tasks/submit"
    TASK_STATUS_URL = "http://host.docker.internal:8000/api/v1/tasks"  # ç”¨äºæŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
    POLL_INTERVAL = 5  # æ¯éš” 5 ç§’è½®è¯¢ä¸€æ¬¡ä»»åŠ¡çŠ¶æ€
    TIMEOUT = 600  # æœ€é•¿ç­‰å¾…æ—¶é—´ 600 ç§’
    print(file_path)
    ROOT_DIR = Path(__file__).resolve().parent.parent.parent   # å‘ä¸Šä¸€çº§
    try:
        input_file = ROOT_DIR / file_path
        task_name = input_file.name  # æå–æ–‡ä»¶åä½œä¸ºä»»åŠ¡å
        if not input_file.exists():
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æäº¤")
            return {"message": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æäº¤"}

        # æäº¤ä»»åŠ¡
        result = graph_base.file_Handle(input_file, EXTERNAL_API_URL)
        if not result or "task_id" not in result:
            print("âŒ æ–‡ä»¶æäº¤å¤±è´¥ï¼Œè¿”å›ç»“æœå¼‚å¸¸")
            return {"message": "æ–‡ä»¶æäº¤å¤±è´¥", "detail": result}

        task_id = result["task_id"]
        print(f"âœ… æ–‡ä»¶æäº¤æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")

        # å¼€å§‹è½®è¯¢ä»»åŠ¡çŠ¶æ€
        start_time = time.time()
        while True:
            # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
            resp = requests.get(f"{TASK_STATUS_URL}/{task_id}", timeout=30)
            resp.raise_for_status()
            status_data = resp.json()
            status = status_data.get("status", "").lower()

            if status == "completed":
                print("âœ… ä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœç»™å‰ç«¯")
                # å¤åˆ¶ output æ–‡ä»¶
                print(task_name)
                copied_file = graph_base.copy_output(task_name)
                print(str(copied_file))
                return {
                    "task_name": task_name,
                    "message": "æ–‡ä»¶å¤„ç†å®Œæˆ",
                    "task_id": task_id,
                    "output_file": str(copied_file),
                    "result": status_data.get("result")  # è¿™é‡Œè¿”å›å®é™…åˆ†æç»“æœ
                }
            elif status == "failed":
                print("âŒ ä»»åŠ¡å¤„ç†å¤±è´¥")
                return {
                    "task_name": task_name,
                    "message": "æ–‡ä»¶å¤„ç†å¤±è´¥",
                    "task_id": task_id,
                    "detail": status_data
                }

            # è¶…æ—¶å¤„ç†
            if time.time() - start_time > TIMEOUT:
                return {
                    "task_name": task_name,
                    "status": "å¤„ç†è¶…æ—¶",
                    "task_id": task_id
                }

            # ç­‰å¾…ä¸‹ä¸€æ¬¡è½®è¯¢
            #å®¹æ˜“æ­»æœº
            time.sleep(POLL_INTERVAL)

    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
        return {"message": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"}


@data.post("/graph/build_graph")
def api_build_graph():
    try:
        response = requests.post(
            "http://host.docker.internal:8111/build_graph",
            json={"clean_copypath": True}
        )

        if response.status_code != 200:
            return {
                "status": "failed",
                "detail": f"è¿œç¨‹æœåŠ¡é”™è¯¯: {response.text}"
            }

        return {
            "status": "success",
            "detail": response.json()
        }

    except Exception as e:
        return {"status": "failed", "detail": str(e)}

@data.post("/graph/build_drillgraph")
def api_build_drillgraph():
    try:
        response = requests.post(
            "http://host.docker.internal:8111/build_drillgraph",
            json={"clean_copypath": True}
        )

        if response.status_code != 200:
            return {
                "status": "failed",
                "detail": f"è¿œç¨‹æœåŠ¡é”™è¯¯: {response.text}"
            }

        return {
            "status": "success",
            "detail": response.json()
        }

    except Exception as e:
        return {"status": "failed", "detail": str(e)}

@data.get("/graph/get_file_list/{graph_type}")
def api_get_file_list(graph_type: str):
    try:

        response = requests.get(
            f"http://host.docker.internal:8111/get_file_list/{graph_type}"
        )
        print(response)
        return Response(
            response.content,
            status_code=response.status_code,
            headers=dict(response.headers)
        )

    except Exception as e:
        return {"status": "failed", "detail": str(e)}

@data.delete("/graph/delete_file/{graph_type}/{file_name}")
def api_delete_graph_file(
    graph_type: str = fastapi.Path(..., description="å›¾è°±ç±»å‹ drill/ground", regex="^(drill|ground)$"),
    file_name: str = fastapi.Path(..., description="è¦åˆ é™¤çš„æ–‡ä»¶å")
):
    """
    åˆ é™¤æŒ‡å®šå›¾è°±ç±»å‹çš„æ–‡ä»¶ï¼ˆä¸­é—´è½¬å‘åˆ°å†…éƒ¨æœåŠ¡ï¼‰
    """
    try:
        # å¯¹æ–‡ä»¶åè¿›è¡Œ URL ç¼–ç 
        from urllib.parse import quote
        encoded_file_name = quote(file_name, safe='')

        # æ„å»ºå†…éƒ¨æœåŠ¡ URL
        target_url = f"http://host.docker.internal:8111/delete_file/{graph_type}/{encoded_file_name}"

        # å‘èµ· DELETE è¯·æ±‚åˆ°å†…éƒ¨æœåŠ¡
        response = requests.delete(target_url)

        # å¦‚æœè¿”å›ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if response.status_code >= 400:
            try:
                detail = response.json().get("detail", response.text)
            except:
                detail = response.text
            raise HTTPException(status_code=response.status_code, detail=detail)

        # æˆåŠŸè¿”å›å†…éƒ¨æœåŠ¡å†…å®¹
        return Response(
            content=response.content,
            status_code=response.status_code,
            headers=dict(response.headers)
        )

    except Exception as e:
        return {"status": "failed", "detail": str(e)}

@data.get("/graph/get_downloadable_files/{graph_type}")
def api_get_downloadable_files(graph_type: str):
    try:

        response = requests.get(
            f"http://host.docker.internal:8111/get_downloadable_files/{graph_type}"
        )

        return Response(
            content=response.content,
            status_code=response.status_code,
            headers=dict(response.headers)
        )

    except Exception as e:
        return {"status": "failed", "detail": str(e)}

@data.get("/graph/download_file/{graph_type}/{file_name}")
async def api_download_file(graph_type: str, file_name: str):
    try:
        print(f"æ¥æ”¶åˆ°ä¸‹è½½è¯·æ±‚ - graph_type: {graph_type}, filename: {file_name}")

        # éªŒè¯ graph_type
        if graph_type not in ["ground", "drill"]:
            return {"status": "failed", "detail": "ä¸æ”¯æŒçš„å›¾è¡¨ç±»å‹"}

        # æ„å»ºä¸Šæ¸¸æœåŠ¡URL
        encoded_filename = quote(file_name, safe='')
        target_url = f"http://host.docker.internal:8111/download_file/{graph_type}/{encoded_filename}"


        # è¯·æ±‚ä¸Šæ¸¸æœåŠ¡ - ä¸è¦ä½¿ç”¨ stream=True
        response = requests.get(target_url)  # ç§»é™¤äº† stream=True

        # å¤„ç†é”™è¯¯å“åº”
        if response.status_code != 200:
            error_msg = f"æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
            if response.status_code in [404, 400]:
                try:
                    error_data = response.json()
                    error_msg = error_data.get('detail', error_msg)
                except:
                    pass
            return {"status": "failed", "detail": error_msg}

        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = len(response.content)
        content_type = response.headers.get('Content-Type', 'application/octet-stream')

        print(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {file_size} bytes")
        print(f"æ–‡ä»¶å†…å®¹é¢„è§ˆ: {response.text[:100]}")  # è°ƒè¯•ï¼šæŸ¥çœ‹æ–‡ä»¶å†…å®¹

        # å¯¹æ–‡ä»¶åè¿›è¡Œ RFC 5987 ç¼–ç ï¼Œæ”¯æŒä¸­æ–‡
        encoded_file_name = quote(file_name, safe='')
        content_disposition = f"attachment; filename*=UTF-8''{encoded_file_name}"

        # ç›´æ¥è¿”å›æ–‡ä»¶å†…å®¹
        return Response(
            content=response.content,
            status_code=200,
            headers={
                'Content-Type': content_type,
                'Content-Disposition': content_disposition,
                'Content-Length': str(file_size),
                'Access-Control-Expose-Headers': 'Content-Disposition'
            },
            media_type=content_type
        )

    except Exception as e:
        print(f"å¼‚å¸¸: {str(e)}")
        return {"status": "failed", "detail": str(e)}


#     graph_type: str = Path(..., description="å›¾è°±ç±»å‹", regex="^(drill|ground)$"),
#     file_name: str = Path(..., description="è¦ä¸‹è½½çš„æ–‡ä»¶å"),
#     current_user: User = Depends(get_admin_user)
# ):
#     """ä¸‹è½½æŒ‡å®šç±»å‹çš„å›¾è°±æ–‡ä»¶"""
#     try:
#         # ç›´æ¥åœ¨å‡½æ•°ä¸­å®šä¹‰è·¯å¾„
#         INDEX_ROOT = Path("/app/indexing")
#         DRILL_INDEX_ROOT = Path("/app/indexing_drill")
#         GROUND_DOWNLOAD_DIR = INDEX_ROOT / "ground_graph_fill"
#         DRILL_DOWNLOAD_DIR = DRILL_INDEX_ROOT / "drill_graph_fill"
#         # æ ¹æ®å›¾è°±ç±»å‹ç¡®å®šç›®æ ‡ç›®å½•
#         if graph_type == "drill":
#             target_dir = DRILL_DOWNLOAD_DIR
#         else:  # ground
#             target_dir = GROUND_DOWNLOAD_DIR
#
#         file_to_download = target_dir / file_name
#
#         # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
#         if not file_to_download.exists():
#             raise HTTPException(
#                 status_code=404,
#                 detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_to_download}"
#             )
#
#         # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ–‡ä»¶
#         if not file_to_download.is_file():
#             raise HTTPException(
#                 status_code=400,
#                 detail=f"'{file_name}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆæ–‡ä»¶"
#             )
#
#         print(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½æ–‡ä»¶: {file_to_download}")
#
#         # è¿”å›æ–‡ä»¶å“åº”
#         return FileResponse(
#             path=file_to_download,
#             filename=file_name,
#             media_type='application/octet-stream'
#         )
#
#     except HTTPException:
#         # é‡æ–°æŠ›å‡ºå·²æœ‰çš„ HTTP å¼‚å¸¸
#         raise
#     except Exception as e:
#         # å¤„ç†å…¶ä»–å¼‚å¸¸
#         raise HTTPException(
#             status_code=500,
#             detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}"
#         ) from e

@data.post("/graph/run_graphrag")
async def run_graphrag_index():
    """
    è§¦å‘ graphrag ç´¢å¼•æ„å»º
    """
    cmd = [
        "docker", "exec",
        "graphrag-worker",
        "python", "-m", "graphrag.index",
        "--root", "./indexing"
    ]
    try:
        subprocess.run(cmd, check=True)
        return {"message": "GraphRAG ç´¢å¼•æ„å»ºæˆåŠŸ"}
    except subprocess.CalledProcessError as e:
        return {"error": f"æ‰§è¡Œå¤±è´¥: {e}"}

@data.post("/update")
async def update_database_info(
    db_id: str = Body(...),
    name: str = Body(...),
    description: str = Body(...),
    current_user: User = Depends(get_admin_user)
):
    logger.debug(f"Update database {db_id} info: {name}, {description}")
    try:
        database = knowledge_base.update_database(db_id, name, description)
        return {"message": "æ›´æ–°æˆåŠŸ", "database": database}
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=400, detail=f"æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")

