# 1. æ­£ç¡®å¯¼å…¥ï¼šåŒºåˆ† FastAPI çš„ Path å’Œ pathlib çš„ Path
from fastapi import FastAPI, HTTPException, Path  # å¯¼å…¥ FastAPI çš„è·¯å¾„å‚æ•°å·¥å…·
from fastapi.responses import FileResponse
import subprocess
import shutil
from pathlib import Path as PathlibPath  # é‡å‘½å pathlib çš„ Pathï¼Œé¿å…æ··æ·†
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import logging
from urllib.parse import unquote
import pyarrow.parquet as pq
import glob
from urllib.parse import quote

app = FastAPI()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. æ–‡ä»¶è·¯å¾„å®šä¹‰ï¼šç”¨é‡å‘½ååçš„ PathlibPathï¼ˆpathlib çš„åŠŸèƒ½ï¼‰
INDEX_ROOT = PathlibPath("/app/indexing")
INPUT_DIR = INDEX_ROOT / "input"  # ground ç›®å½•
COPYPATH_ROOT = PathlibPath("/app/saves/data/copypath")
DRILL_INDEX_ROOT = PathlibPath("/app/indexing_drill")
DRILL_INPUT_DIR = DRILL_INDEX_ROOT / "input"  # drill ç›®å½•
# å®šä¹‰å¯ä¸‹è½½æ–‡ä»¶çš„ç›®å½•
GROUND_DOWNLOAD_DIR = INDEX_ROOT / "ground_graph_fill"
DRILL_DOWNLOAD_DIR = DRILL_INDEX_ROOT / "drill_graph_fill"
DIAMAGNETIC_INPUT_DIR = PathlibPath("/app/indexing_diamagnetic/input")


def convert_to_graph_format(input_csv_path: PathlibPath, output_csv_path: PathlibPath) -> dict:
    """
    å°†çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è½¬æ¢ä¸ºå›¾æ•°æ®åº“ä¸Šä¼ æ ¼å¼

    Args:
        input_csv_path: è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„
        output_csv_path: è¾“å‡ºçš„CSVæ–‡ä»¶è·¯å¾„

    Returns:
        dict: è½¬æ¢ç»“æœä¿¡æ¯
    """
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not input_csv_path.exists():
            return {"status": "error", "detail": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_csv_path}"}

        # è¯»å–CSVæ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {input_csv_path}")
        df = pd.read_csv(input_csv_path)

        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['source', 'target', 'description']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            return {
                "status": "error",
                "detail": f"CSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}",
                "available_columns": list(df.columns)
            }

        # æå–éœ€è¦çš„åˆ—å¹¶é‡å‘½å
        graph_df = df[['source', 'description', 'target']].copy()
        graph_df.columns = ['h', 'r', 't']  # é‡å‘½åä¸ºå›¾æ•°æ®åº“è¦æ±‚çš„æ ¼å¼

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_csv_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºæ–°çš„CSVæ–‡ä»¶
        graph_df.to_csv(output_csv_path, index=False, encoding='utf-8')

        print(f"âœ… å·²æˆåŠŸè½¬æ¢æ–‡ä»¶æ ¼å¼")
        print(f"ğŸ“Š è½¬æ¢ç»Ÿè®¡: {len(graph_df)} æ¡å…³ç³»")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_csv_path}")

        return {
            "status": "success",
            "detail": "æ–‡ä»¶æ ¼å¼è½¬æ¢æˆåŠŸ",
            "input_file": str(input_csv_path),
            "output_file": str(output_csv_path),
            "relationship_count": len(graph_df),
            "sample_data": graph_df.head(3).to_dict('records')  # è¿”å›å‰3æ¡æ•°æ®ä½œä¸ºç¤ºä¾‹
        }

    except Exception as e:
        return {"status": "error", "detail": f"æ–‡ä»¶æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}"}

def convert_parquet_to_csv(parquet_path: PathlibPath, csv_path: PathlibPath) -> dict:
    """
    å°† Parquet æ–‡ä»¶è½¬æ¢ä¸º CSV æ–‡ä»¶

    Args:
        parquet_path: Parquet æ–‡ä»¶è·¯å¾„
        csv_path: è¾“å‡ºçš„ CSV æ–‡ä»¶è·¯å¾„

    Returns:
        dict: è½¬æ¢ç»“æœä¿¡æ¯
    """
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not parquet_path.exists():
            return {"status": "error", "detail": f"Parquet æ–‡ä»¶ä¸å­˜åœ¨: {parquet_path}"}

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        csv_path.parent.mkdir(parents=True, exist_ok=True)

        # è¯»å– Parquet æ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å– Parquet æ–‡ä»¶: {parquet_path}")
        df = pd.read_parquet(parquet_path)

        # è½¬æ¢ä¸º CSV
        print(f"ğŸ’¾ æ­£åœ¨å†™å…¥ CSV æ–‡ä»¶: {csv_path}")
        df.to_csv(csv_path, index=False, encoding='utf-8')

        # è¿”å›è½¬æ¢ä¿¡æ¯
        return {
            "status": "success",
            "detail": "æ–‡ä»¶è½¬æ¢æˆåŠŸ",
            "original_file": str(parquet_path),
            "converted_file": str(csv_path),
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": list(df.columns)
        }

    except Exception as e:
        return {"status": "error", "detail": f"æ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}"}


def find_latest_output_dir(base_dir: PathlibPath) -> PathlibPath:
    """
    åœ¨ output ç›®å½•ä¸­æ‰¾åˆ°æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•

    Args:
        base_dir: åŸºç¡€ç›®å½•ï¼ˆINDEX_ROOT æˆ– DRILL_INDEX_ROOTï¼‰

    Returns:
        PathlibPath: æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•è·¯å¾„
    """
    output_dir = base_dir / "output"
    if not output_dir.exists():
        return None

    # è·å–æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•å¹¶æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    timestamp_dirs = sorted([d for d in output_dir.iterdir() if d.is_dir()], reverse=True)

    if not timestamp_dirs:
        return None

    return timestamp_dirs[0]
# ------------------- åŸæœ‰æ¥å£ï¼ˆæ— è·¯å¾„å‚æ•°ï¼Œæ— éœ€ä¿®æ”¹ï¼‰-------------------
@app.post("/init_index")
def init_index():
    """åˆ›å»ºç´¢å¼•ï¼ˆä¿æŒä¸å˜ï¼‰"""
    try:
        subprocess.run(
            ["python", "-m", "graphrag.index", "--init", "--root", str(INDEX_ROOT)],
            check=True
        )
        return {"status": "ç´¢å¼•åˆ›å»ºæˆåŠŸ"}
    except subprocess.CalledProcessError as e:
        return {"status": "ç´¢å¼•åˆ›å»ºå¤±è´¥", "detail": str(e)}


@app.post("/build_graph")
def build_graph(clean_copypath: bool = True):
    """æ„å»º ground å›¾è°±"""
    try:
        INPUT_DIR.mkdir(parents=True, exist_ok=True)
        print(f"âœ… ç¡®ä¿ input ç›®å½•å­˜åœ¨: {INPUT_DIR}")

        if not COPYPATH_ROOT.exists():
            raise HTTPException(status_code=400, detail=f"copypath ç›®å½•ä¸å­˜åœ¨: {COPYPATH_ROOT}")

        copypath_files = [f for f in COPYPATH_ROOT.iterdir() if f.is_file()]
        if not copypath_files:
            raise HTTPException(status_code=400, detail=f"copypath ç›®å½•ä¸­æ— æ–‡ä»¶å¯å¤åˆ¶: {COPYPATH_ROOT}")

        for file in copypath_files:
            target_file = INPUT_DIR / file.name
            shutil.copy2(file, target_file)
            print(f"ğŸ“ å·²å¤åˆ¶æ–‡ä»¶: {file.name} â†’ {INPUT_DIR}")

        print("âœ… å›¾è°±æ„å»ºå³å°†æ‰§è¡Œ")
        subprocess.run(
            ["python", "-m", "graphrag.index", "--root", str(INDEX_ROOT)],
            check=True,
        )
        print("âœ… å›¾è°±æ„å»ºå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")

        # æ–°å¢ï¼šæŸ¥æ‰¾å¹¶è½¬æ¢ Parquet æ–‡ä»¶
        conversion_result = None
        graph_conversion_result = None

        # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
        GROUND_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)

        # æŸ¥æ‰¾æœ€æ–°çš„è¾“å‡ºç›®å½•
        latest_output_dir = find_latest_output_dir(INDEX_ROOT)

        if latest_output_dir:
            parquet_file = latest_output_dir / "artifacts" / "create_final_relationships.parquet"
            if parquet_file.exists():
                # ç”Ÿæˆ CSV æ–‡ä»¶å
                csv_filename = "create_final_relationships.csv"
                csv_file = GROUND_DOWNLOAD_DIR / csv_filename

                # è½¬æ¢æ–‡ä»¶
                conversion_result = convert_parquet_to_csv(parquet_file, csv_file)
                print(f"ğŸ“Š Ground å›¾è°±è½¬æ¢ç»“æœ: {conversion_result['status']}")

                # å¦‚æœè½¬æ¢æˆåŠŸï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯å¹¶è¿›è¡Œå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢
                if conversion_result["status"] == "success":
                    print(f"âœ… å·²è½¬æ¢: {parquet_file} â†’ {csv_file}")
                    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {conversion_result['rows']} è¡Œ, {conversion_result['columns']} åˆ—")

                    # ç›´æ¥è¿›è¡Œå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢
                    graph_csv_filename = "graph_format_relationships.csv"
                    graph_csv_file = GROUND_DOWNLOAD_DIR / graph_csv_filename

                    graph_conversion_result = convert_to_graph_format(csv_file, graph_csv_file)
                    print(f"ğŸ“Š å›¾æ•°æ®åº“æ ¼å¼è½¬æ¢ç»“æœ: {graph_conversion_result['status']}")

                    # å¦‚æœå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢æˆåŠŸï¼Œåˆ é™¤åŸå§‹çš„CSVæ–‡ä»¶ï¼Œåªä¿ç•™è§„èŒƒåŒ–çš„æ–‡ä»¶
                    if graph_conversion_result["status"] == "success":
                        # åˆ é™¤åŸå§‹çš„CSVæ–‡ä»¶
                        csv_file.unlink()
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤åŸå§‹CSVæ–‡ä»¶: {csv_file}")

                        # å°†è§„èŒƒåŒ–æ–‡ä»¶é‡å‘½åä¸ºåŸå§‹æ–‡ä»¶å
                        final_csv_file = GROUND_DOWNLOAD_DIR / csv_filename
                        graph_csv_file.rename(final_csv_file)
                        print(f"ğŸ“ å·²å°†è§„èŒƒåŒ–æ–‡ä»¶é‡å‘½åä¸º: {final_csv_file}")

                        # æ›´æ–°è½¬æ¢ç»“æœä¿¡æ¯
                        conversion_result["converted_file"] = str(final_csv_file)
                        conversion_result["detail"] = "æ–‡ä»¶å·²è½¬æ¢ä¸ºå›¾æ•°æ®åº“æ ¼å¼å¹¶ä¿å­˜"
            else:
                conversion_result = {"status": "warning", "detail": f"æœªæ‰¾åˆ° Parquet æ–‡ä»¶: {parquet_file}"}
                print(f"âš ï¸ {conversion_result['detail']}")

                # åˆ—å‡º artifacts ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
                artifacts_dir = latest_output_dir / "artifacts"
                if artifacts_dir.exists():
                    artifact_files = [f.name for f in artifacts_dir.iterdir() if f.is_file()]
                    print(f"ğŸ“ artifacts ç›®å½•ä¸­çš„æ–‡ä»¶: {artifact_files}")
        else:
            conversion_result = {"status": "warning", "detail": "æœªæ‰¾åˆ°æœ€æ–°çš„ output ç›®å½•"}
            print(f"âš ï¸ {conversion_result['detail']}")

            # åˆ—å‡º output ç›®å½•ä¸­çš„æ‰€æœ‰å­ç›®å½•ï¼Œæ–¹ä¾¿è°ƒè¯•
            output_dir = INDEX_ROOT / "output"
            if output_dir.exists():
                subdirs = [d.name for d in output_dir.iterdir() if d.is_dir()]
                print(f"ğŸ“ output ç›®å½•ä¸­çš„å­ç›®å½•: {subdirs}")

        if clean_copypath:
            for file in copypath_files:
                file.unlink()
            print(f"âœ… å·²æ¸…ç©º copypath ç›®å½•: {COPYPATH_ROOT}")

        response_data = {
            "status": "å›¾è°±æ„å»ºæˆåŠŸ",
            "detail": f"å·²å¤åˆ¶ {len(copypath_files)} ä¸ªæ–‡ä»¶åˆ° indexing/input"
        }

        if conversion_result:
            response_data["csv_conversion"] = conversion_result

        if graph_conversion_result:
            response_data["graph_format_conversion"] = graph_conversion_result

        return response_data

    except subprocess.CalledProcessError as e:
        return {
            "status": "å›¾è°±æ„å»ºå¤±è´¥",
            "detail": f"å‘½ä»¤æ‰§è¡Œé”™è¯¯: {str(e)}",
            "é”™è¯¯è¾“å‡º": e.stderr.decode('utf-8') if e.stderr else "æ— è¯¦ç»†é”™è¯¯ä¿¡æ¯"
        }
    except Exception as e:
        return {"status": "æ“ä½œå¤±è´¥", "detail": str(e)}


@app.post("/build_drillgraph")
def build_drillgraph(clean_copypath: bool = True):
    """æ„å»º drill å›¾è°±"""
    try:
        DRILL_INPUT_DIR.mkdir(parents=True, exist_ok=True)
        print(f"âœ… ç¡®ä¿ drill input ç›®å½•å­˜åœ¨: {DRILL_INPUT_DIR}")

        if not COPYPATH_ROOT.exists():
            raise HTTPException(status_code=400, detail=f"copypath ç›®å½•ä¸å­˜åœ¨: {COPYPATH_ROOT}")

        copypath_files = [f for f in COPYPATH_ROOT.iterdir() if f.is_file()]
        if not copypath_files:
            raise HTTPException(status_code=400, detail=f"copypath ç›®å½•ä¸­æ— æ–‡ä»¶å¯å¤åˆ¶: {COPYPATH_ROOT}")

        for file in copypath_files:
            target_file = DRILL_INPUT_DIR / file.name
            shutil.copy2(file, target_file)
            print(f"ğŸ“ å·²å¤åˆ¶æ–‡ä»¶: {file.name} â†’ {DRILL_INPUT_DIR}")

        print("âœ… Drill å›¾è°±æ„å»ºå³å°†æ‰§è¡Œ")
        subprocess.run(
            ["python", "-m", "graphrag.index", "--root", str(DRILL_INDEX_ROOT)],
            check=True,
        )
        print("âœ… Drill å›¾è°±æ„å»ºå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")

        # æ–°å¢ï¼šæŸ¥æ‰¾å¹¶è½¬æ¢ Parquet æ–‡ä»¶
        conversion_result = None
        graph_conversion_result = None

        # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
        DRILL_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)

        # æŸ¥æ‰¾æœ€æ–°çš„è¾“å‡ºç›®å½•
        latest_output_dir = find_latest_output_dir(DRILL_INDEX_ROOT)

        if latest_output_dir:
            parquet_file = latest_output_dir / "artifacts" / "create_final_relationships.parquet"
            if parquet_file.exists():
                # ç”Ÿæˆ CSV æ–‡ä»¶å
                csv_filename = "create_final_relationships.csv"
                csv_file = DRILL_DOWNLOAD_DIR / csv_filename

                # è½¬æ¢æ–‡ä»¶
                conversion_result = convert_parquet_to_csv(parquet_file, csv_file)
                print(f"ğŸ“Š Drill å›¾è°±è½¬æ¢ç»“æœ: {conversion_result['status']}")

                # å¦‚æœè½¬æ¢æˆåŠŸï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯å¹¶è¿›è¡Œå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢
                if conversion_result["status"] == "success":
                    print(f"âœ… å·²è½¬æ¢: {parquet_file} â†’ {csv_file}")
                    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {conversion_result['rows']} è¡Œ, {conversion_result['columns']} åˆ—")

                    # ç›´æ¥è¿›è¡Œå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢
                    graph_csv_filename = "graph_format_relationships.csv"
                    graph_csv_file = DRILL_DOWNLOAD_DIR / graph_csv_filename

                    graph_conversion_result = convert_to_graph_format(csv_file, graph_csv_file)
                    print(f"ğŸ“Š å›¾æ•°æ®åº“æ ¼å¼è½¬æ¢ç»“æœ: {graph_conversion_result['status']}")

                    # å¦‚æœå›¾æ•°æ®åº“æ ¼å¼è½¬æ¢æˆåŠŸï¼Œåˆ é™¤åŸå§‹çš„CSVæ–‡ä»¶ï¼Œåªä¿ç•™è§„èŒƒåŒ–çš„æ–‡ä»¶
                    if graph_conversion_result["status"] == "success":
                        # åˆ é™¤åŸå§‹çš„CSVæ–‡ä»¶
                        csv_file.unlink()
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤åŸå§‹CSVæ–‡ä»¶: {csv_file}")

                        # å°†è§„èŒƒåŒ–æ–‡ä»¶é‡å‘½åä¸ºåŸå§‹æ–‡ä»¶å
                        final_csv_file = DRILL_DOWNLOAD_DIR / csv_filename
                        graph_csv_file.rename(final_csv_file)
                        print(f"ğŸ“ å·²å°†è§„èŒƒåŒ–æ–‡ä»¶é‡å‘½åä¸º: {final_csv_file}")

                        # æ›´æ–°è½¬æ¢ç»“æœä¿¡æ¯
                        conversion_result["converted_file"] = str(final_csv_file)
                        conversion_result["detail"] = "æ–‡ä»¶å·²è½¬æ¢ä¸ºå›¾æ•°æ®åº“æ ¼å¼å¹¶ä¿å­˜"
            else:
                conversion_result = {"status": "warning", "detail": f"æœªæ‰¾åˆ° Parquet æ–‡ä»¶: {parquet_file}"}
                print(f"âš ï¸ {conversion_result['detail']}")

                # åˆ—å‡º artifacts ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
                artifacts_dir = latest_output_dir / "artifacts"
                if artifacts_dir.exists():
                    artifact_files = [f.name for f in artifacts_dir.iterdir() if f.is_file()]
                    print(f"ğŸ“ artifacts ç›®å½•ä¸­çš„æ–‡ä»¶: {artifact_files}")
        else:
            conversion_result = {"status": "warning", "detail": "æœªæ‰¾åˆ°æœ€æ–°çš„ output ç›®å½•"}
            print(f"âš ï¸ {conversion_result['detail']}")

            # åˆ—å‡º output ç›®å½•ä¸­çš„æ‰€æœ‰å­ç›®å½•ï¼Œæ–¹ä¾¿è°ƒè¯•
            output_dir = DRILL_INDEX_ROOT / "output"
            if output_dir.exists():
                subdirs = [d.name for d in output_dir.iterdir() if d.is_dir()]
                print(f"ğŸ“ output ç›®å½•ä¸­çš„å­ç›®å½•: {subdirs}")

        if clean_copypath:
            for file in copypath_files:
                file.unlink()
            print(f"âœ… å·²æ¸…ç©º copypath ç›®å½•: {COPYPATH_ROOT}")

        response_data = {
            "status": "Drill å›¾è°±æ„å»ºæˆåŠŸ",
            "detail": f"å·²å¤åˆ¶ {len(copypath_files)} ä¸ªæ–‡ä»¶åˆ° indexing_drill/input"
        }

        if conversion_result:
            response_data["csv_conversion"] = conversion_result

        if graph_conversion_result:
            response_data["graph_format_conversion"] = graph_conversion_result

        return response_data

    except subprocess.CalledProcessError as e:
        return {
            "status": "Drill å›¾è°±æ„å»ºå¤±è´¥",
            "detail": f"å‘½ä»¤æ‰§è¡Œé”™è¯¯: {str(e)}",
            "é”™è¯¯è¾“å‡º": e.stderr.decode('utf-8') if e.stderr else "æ— è¯¦ç»†é”™è¯¯ä¿¡æ¯"
        }
    except Exception as e:
        return {"status": "æ“ä½œå¤±è´¥", "detail": str(e)}


# ------------------- ä¿®æ­£è·¯å¾„å‚æ•°çš„æ¥å£ -------------------
@app.get("/get_file_list/{directory_type}")
def get_file_list(
        # ç”¨ FastAPI çš„ Path å®šä¹‰è·¯å¾„å‚æ•°ï¼ˆå¿…å¡«ã€æè¿°ã€æ­£åˆ™éªŒè¯ï¼‰
        directory_type: str = Path(..., description="è¦æŸ¥è¯¢çš„ç›®å½•ç±»å‹", regex="^(drill|ground)$")
):
    """è·å–æŒ‡å®šç›®å½•ï¼ˆdrill/groundï¼‰ä¸‹çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        # ç”¨ PathlibPath å¤„ç†æ–‡ä»¶è·¯å¾„
        if directory_type == "drill":
            target_dir = DRILL_INPUT_DIR
        else:  # ground
            target_dir = INPUT_DIR

        if not target_dir.exists() or not target_dir.is_dir():
            raise HTTPException(status_code=404, detail=f"{directory_type} ç›®å½•ä¸å­˜åœ¨: {target_dir}")

        file_list = []
        for file_path in target_dir.iterdir():
            if file_path.is_file():
                file_list.append({
                    "file_name": file_path.name,
                    "size_bytes": file_path.stat().st_size
                })

        return {
            "status": "success",
            "directory": str(target_dir),
            "file_count": len(file_list),
            "files": file_list
        }

    except HTTPException:
        raise
    except Exception as e:
        return {"status": "error", "detail": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"}

@app.delete("/delete_file/{directory_type}/{file_name}")
def delete_specific_file(
        directory_type: str = Path(..., description="æ–‡ä»¶æ‰€åœ¨ç›®å½•ç±»å‹", regex="^(drill|ground|diamagnetic)$"),
        file_name: str = Path(..., description="è¦åˆ é™¤çš„æ–‡ä»¶å")
):
    """
    åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶ï¼ŒåŒæ—¶å¯è½¬å‘åˆ°å†…éƒ¨æœåŠ¡ã€‚
    """
    from urllib.parse import unquote
    file_name = unquote(file_name)  # è§£å†³ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦é—®é¢˜
    # æ‰“å°æ¥æ”¶åˆ°çš„å‚æ•°
    logger.info(f"ğŸ”¹ directory_type: {directory_type}")
    logger.info(f"ğŸ”¹ file_name (decoded): {file_name}")
    try:
        # é€‰æ‹©ç›®æ ‡ç›®å½•
        if directory_type == "drill":
            target_dir = DRILL_INPUT_DIR
        elif directory_type == "ground":
            target_dir = INPUT_DIR
        else:  # diamagnetic
            target_dir = DIAMAGNETIC_INPUT_DIR


        file_to_delete = target_dir / file_name

        if not file_to_delete.exists() or not file_to_delete.is_file():
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_to_delete}")

        # åˆ é™¤æœ¬åœ°æ–‡ä»¶
        file_to_delete.unlink()
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶: {file_to_delete}")



        return {
            "status": "success",
            "detail": f"æ–‡ä»¶ '{file_name}' å·²ä» '{directory_type}' ç›®å½•ä¸­åˆ é™¤"
        }

    except HTTPException:
        raise
    except Exception as e:
        return {"status": "error", "detail": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"}

@app.get("/get_downloadable_files/{directory_type}")
def get_downloadable_files(
    directory_type: str = Path(..., description="è¦æŸ¥è¯¢çš„ç›®å½•ç±»å‹", regex="^(drill|ground)$")
):
    """
    è·å–æŒ‡å®šç±»å‹ï¼ˆdrill/groundï¼‰å¯¹åº”çš„å¯ä¸‹è½½æ–‡ä»¶åˆ—è¡¨ã€‚
    """
    try:
        # æ ¹æ®ç±»å‹é€‰æ‹©å¯¹åº”çš„ä¸‹è½½ç›®å½•
        if directory_type == "drill":
            target_dir = DRILL_DOWNLOAD_DIR
        else:  # ground
            target_dir = GROUND_DOWNLOAD_DIR

        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not target_dir.exists() or not target_dir.is_dir():
            # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œè€Œä¸æ˜¯æŠ¥é”™ï¼Œè¿™æ ·å‰ç«¯ä½“éªŒæ›´å¥½
            # ä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦æ”¹ä¸º raise HTTPException
            return {
                "status": "success",
                "directory": str(target_dir),
                "file_count": 0,
                "files": []
            }

        # éå†ç›®å½•ï¼Œè·å–æ–‡ä»¶ä¿¡æ¯
        file_list = []
        for file_path in target_dir.iterdir():
            if file_path.is_file():
                file_list.append({
                    "file_name": file_path.name,
                    "size_bytes": file_path.stat().st_size
                })

        return {
            "status": "success",
            "directory": str(target_dir),
            "file_count": len(file_list),
            "files": file_list
        }

    except HTTPException:
        raise
    except Exception as e:
        return {"status": "error", "detail": f"è·å–å¯ä¸‹è½½æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"}
# @app.delete("/delete_specific_file/{directory_type}/{file_name}")
# def delete_specific_file(
#         directory_type: str = Path(..., description="æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ç±»å‹", regex="^(drill|ground)$"),
#         file_name: str = Path(..., description="è¦åˆ é™¤çš„æ–‡ä»¶å")
# ):
#     """åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶"""
#     try:
#         if directory_type == "drill":
#             target_dir = DRILL_INPUT_DIR
#         else:  # ground
#             target_dir = INPUT_DIR
#
#         file_to_delete = target_dir / file_name
#
#         if not file_to_delete.exists() or not file_to_delete.is_file():
#             raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_to_delete}")
#
#         file_to_delete.unlink()
#         print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ–‡ä»¶: {file_to_delete}")
#
#         return {
#             "status": "success",
#             "detail": f"æ–‡ä»¶ '{file_name}' å·²ä» '{directory_type}' ç›®å½•ä¸­æˆåŠŸåˆ é™¤ã€‚"
#         }
#
#     except HTTPException:
#         raise
#     except Exception as e:
#         return {"status": "error", "detail": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"}
#
#
# # ... (ç¡®ä¿ä½ å·²ç»å®šä¹‰äº†è¿™ä¸¤ä¸ªä¸‹è½½ç›®å½•)
# # GROUND_DOWNLOAD_DIR = INDEX_ROOT / "ground_graph_fill"
# # DRILL_DOWNLOAD_DIR = DRILL_INDEX_ROOT / "drill_graph_fill"

@app.get("/download_file/{directory_type}/{file_name}")
def download_file(
    directory_type: str = Path(..., description="æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ç±»å‹", regex="^(drill|ground)$"),
    file_name: str = Path(..., description="è¦ä¸‹è½½çš„æ–‡ä»¶å")
):
    """ä¸‹è½½æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶"""
    try:
        if directory_type == "drill":
            target_dir = DRILL_DOWNLOAD_DIR
        else:  # ground
            target_dir = GROUND_DOWNLOAD_DIR

        file_to_download = target_dir / file_name

        if not file_to_download.exists():
            # è¿”å› 404 é”™è¯¯å’Œ JSON æ ¼å¼çš„é”™è¯¯ä¿¡æ¯
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_to_download}")

        if not file_to_download.is_file():
            # è¿”å› 400 é”™è¯¯å’Œ JSON æ ¼å¼çš„é”™è¯¯ä¿¡æ¯
            raise HTTPException(status_code=400, detail=f"'{file_name}' ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ã€‚")

        print(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½æ–‡ä»¶: {file_to_download}")

        return FileResponse(
            path=file_to_download,
            filename=file_name,
            media_type='application/octet-stream'
        )

    except HTTPException:
        raise
    except Exception as e:
        # è¿”å› 500 é”™è¯¯å’Œ JSON æ ¼å¼çš„é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}") from e


