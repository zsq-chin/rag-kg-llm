"""
MinerU Tianshu - API Server
å¤©æ¢ API æœåŠ¡å™¨

ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å°
æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†
æä¾› RESTful API æ¥å£ç”¨äºä»»åŠ¡æäº¤ã€æŸ¥è¯¢å’Œç®¡ç†
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import tempfile
from pathlib import Path
from loguru import logger
import uvicorn
from typing import Optional
from datetime import datetime
import os
import re
import uuid
from minio import Minio

from task_db import TaskDB

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="MinerU Tianshu API",
    description="å¤©æ¢ - ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å° | æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†",
    version="1.0.0"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ•°æ®åº“
db = TaskDB()

# é…ç½®è¾“å‡ºç›®å½•
OUTPUT_DIR = Path('/tmp/mineru_tianshu_output')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# MinIO é…ç½®
MINIO_CONFIG = {
    'endpoint': os.getenv('MINIO_ENDPOINT', ''),
    'access_key': os.getenv('MINIO_ACCESS_KEY', ''),
    'secret_key': os.getenv('MINIO_SECRET_KEY', ''),
    'secure': True,
    'bucket_name': os.getenv('MINIO_BUCKET', '')
}


def get_minio_client():
    """è·å–MinIOå®¢æˆ·ç«¯å®ä¾‹"""
    return Minio(
        MINIO_CONFIG['endpoint'],
        access_key=MINIO_CONFIG['access_key'],
        secret_key=MINIO_CONFIG['secret_key'],
        secure=MINIO_CONFIG['secure']
    )


def process_markdown_images(md_content: str, image_dir: Path, upload_images: bool = False):
    """
    å¤„ç† Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨

    Args:
        md_content: Markdown å†…å®¹
        image_dir: å›¾ç‰‡æ‰€åœ¨ç›®å½•
        upload_images: æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ° MinIO å¹¶æ›¿æ¢é“¾æ¥

    Returns:
        å¤„ç†åçš„ Markdown å†…å®¹
    """
    if not upload_images:
        return md_content

    try:
        minio_client = get_minio_client()
        bucket_name = MINIO_CONFIG['bucket_name']
        minio_endpoint = MINIO_CONFIG['endpoint']

        # æŸ¥æ‰¾æ‰€æœ‰ markdown æ ¼å¼çš„å›¾ç‰‡
        img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'

        def replace_image(match):
            alt_text = match.group(1)
            image_path = match.group(2)

            # æ„å»ºå®Œæ•´çš„æœ¬åœ°å›¾ç‰‡è·¯å¾„
            full_image_path = image_dir / Path(image_path).name

            if full_image_path.exists():
                # è·å–æ–‡ä»¶åç¼€
                file_extension = full_image_path.suffix
                # ç”Ÿæˆ UUID ä½œä¸ºæ–°æ–‡ä»¶å
                new_filename = f"{uuid.uuid4()}{file_extension}"

                try:
                    # ä¸Šä¼ åˆ° MinIO
                    object_name = f"images/{new_filename}"
                    minio_client.fput_object(bucket_name, object_name, str(full_image_path))

                    # ç”Ÿæˆ MinIO è®¿é—® URL
                    scheme = 'https' if MINIO_CONFIG['secure'] else 'http'
                    minio_url = f"{scheme}://{minio_endpoint}/{bucket_name}/{object_name}"

                    # è¿”å› HTML æ ¼å¼çš„ img æ ‡ç­¾
                    return f'<img src="{minio_url}" alt="{alt_text}">'
                except Exception as e:
                    logger.error(f"Failed to upload image to MinIO: {e}")
                    return match.group(0)  # ä¸Šä¼ å¤±è´¥ï¼Œä¿æŒåŸæ ·

            return match.group(0)

        # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        new_content = re.sub(img_pattern, replace_image, md_content)
        return new_content

    except Exception as e:
        logger.error(f"Error processing markdown images: {e}")
        return md_content  # å‡ºé”™æ—¶è¿”å›åŸå†…å®¹


@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "service": "MinerU Tianshu",
        "version": "1.0.0",
        "description": "å¤©æ¢ - ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å°",
        "features": "æ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†",
        "docs": "/docs"
    }


@app.post("/api/v1/tasks/submit")
async def submit_task(
    file: UploadFile = File(..., description="æ–‡ä»¶: PDF/å›¾ç‰‡/Office/HTML/éŸ³é¢‘/è§†é¢‘ç­‰å¤šç§æ ¼å¼"),
    backend: str = Form('pipeline', description="å¤„ç†åç«¯: pipeline/deepseek-ocr/paddleocr-vl (æ–‡æ¡£) | sensevoice (éŸ³é¢‘) | video (è§†é¢‘)"),
    lang: str = Form('auto', description="è¯­è¨€: auto/ch/en/korean/japanç­‰"),
    method: str = Form('auto', description="è§£ææ–¹æ³•: auto/txt/ocr"),
    formula_enable: bool = Form(True, description="æ˜¯å¦å¯ç”¨å…¬å¼è¯†åˆ«"),
    table_enable: bool = Form(True, description="æ˜¯å¦å¯ç”¨è¡¨æ ¼è¯†åˆ«"),
    priority: int = Form(0, description="ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§è¶Šä¼˜å…ˆ"),
    # DeepSeek OCR ä¸“ç”¨å‚æ•°
    deepseek_resolution: str = Form('base', description="DeepSeek OCR åˆ†è¾¨ç‡: tiny/small/base/large/dynamic"),
    deepseek_prompt_type: str = Form('document', description="DeepSeek OCR æç¤ºè¯ç±»å‹: document/image/free/figure"),
    # è§†é¢‘å¤„ç†ä¸“ç”¨å‚æ•°
    keep_audio: bool = Form(False, description="è§†é¢‘å¤„ç†æ—¶æ˜¯å¦ä¿ç•™æå–çš„éŸ³é¢‘æ–‡ä»¶"),
    enable_keyframe_ocr: bool = Form(False, description="æ˜¯å¦å¯ç”¨è§†é¢‘å…³é”®å¸§OCRè¯†åˆ«ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰"),
    ocr_backend: str = Form('paddleocr-vl', description="å…³é”®å¸§OCRå¼•æ“: paddleocr-vl/deepseek-ocr"),
    keep_keyframes: bool = Form(False, description="æ˜¯å¦ä¿ç•™æå–çš„å…³é”®å¸§å›¾åƒ"),
    # æ°´å°å»é™¤ä¸“ç”¨å‚æ•°
    remove_watermark: bool = Form(False, description="æ˜¯å¦å¯ç”¨æ°´å°å»é™¤ï¼ˆæ”¯æŒ PDF/å›¾ç‰‡ï¼‰"),
    watermark_conf_threshold: float = Form(0.35, description="æ°´å°æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼Œæ¨è 0.35ï¼‰"),
    watermark_dilation: int = Form(10, description="æ°´å°æ©ç è†¨èƒ€å¤§å°ï¼ˆåƒç´ ï¼Œæ¨è 10ï¼‰"),
):
    """
    æäº¤æ–‡æ¡£è§£æä»»åŠ¡

    ç«‹å³è¿”å› task_idï¼Œä»»åŠ¡åœ¨åå°å¼‚æ­¥å¤„ç†
    """
    try:
        # âœ… å®šä¹‰ç›®æ ‡ä¿å­˜ç›®å½•
        save_dir = Path("/app/saves/data/graphragfile")
        save_dir.mkdir(parents=True, exist_ok=True)
        # âœ… æ„é€ æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆä¿æŒåŸæ–‡ä»¶åï¼‰
        save_path = save_dir / file.filename
        # æµå¼å†™å…¥æ–‡ä»¶åˆ°ç£ç›˜ï¼Œé¿å…é«˜å†…å­˜ä½¿ç”¨
        with open(save_path, "wb") as f:
            while True:
                chunk = await file.read(1 << 23)  # 8MB per chunk
                if not chunk:
                    break
                f.write(chunk)

        # åˆ›å»ºä»»åŠ¡
        task_id = db.create_task(
            file_name=file.filename,
            file_path=str(save_path),
            backend=backend,
            options={
                'lang': lang,
                'method': method,
                'formula_enable': formula_enable,
                'table_enable': table_enable,
                # DeepSeek OCR å‚æ•°
                'deepseek_resolution': deepseek_resolution,
                'deepseek_prompt_type': deepseek_prompt_type,
                # è§†é¢‘å¤„ç†å‚æ•°
                'keep_audio': keep_audio,
                'enable_keyframe_ocr': enable_keyframe_ocr,
                'ocr_backend': ocr_backend,
                'keep_keyframes': keep_keyframes,
                # æ°´å°å»é™¤å‚æ•°
                'remove_watermark': remove_watermark,
                'watermark_conf_threshold': watermark_conf_threshold,
                'watermark_dilation': watermark_dilation,
            },
            priority=priority
        )

        logger.info(f"âœ… Task submitted: {task_id} - {file.filename}")
        logger.info(f"   Backend: {backend}")
        logger.info(f"   Priority: {priority}")
        if backend == 'deepseek-ocr':
            logger.info(f"   DeepSeek Resolution: {deepseek_resolution}")
            logger.info(f"   DeepSeek Prompt Type: {deepseek_prompt_type}")

        return {
            'success': True,
            'task_id': task_id,
            'status': 'pending',
            'message': 'Task submitted successfully',
            'file_name': file.filename,
            'created_at': datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ Failed to submit task: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(
    task_id: str,
    upload_images: bool = Query(False, description="æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ°MinIOå¹¶æ›¿æ¢é“¾æ¥ï¼ˆä»…å½“ä»»åŠ¡å®Œæˆæ—¶æœ‰æ•ˆï¼‰"),
    format: str = Query('markdown', description="è¿”å›æ ¼å¼: markdown(é»˜è®¤)/json/both")
):
    """
    æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€å’Œè¯¦æƒ…

    å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œä¼šè‡ªåŠ¨è¿”å›è§£æåçš„å†…å®¹ï¼ˆdata å­—æ®µï¼‰
    - format=markdown: åªè¿”å› Markdown å†…å®¹ï¼ˆé»˜è®¤ï¼‰
    - format=json: åªè¿”å› JSON ç»“æ„åŒ–æ•°æ®ï¼ˆMinerU å’Œ PaddleOCR-VL æ”¯æŒï¼‰
    - format=both: åŒæ—¶è¿”å› Markdown å’Œ JSON
    å¯é€‰æ‹©æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ° MinIO å¹¶æ›¿æ¢ä¸º URL
    """
    task = db.get_task(task_id)

    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    response = {
        'success': True,
        'task_id': task_id,
        'status': task['status'],
        'file_name': task['file_name'],
        'backend': task['backend'],
        'priority': task['priority'],
        'error_message': task['error_message'],
        'created_at': task['created_at'],
        'started_at': task['started_at'],
        'completed_at': task['completed_at'],
        'worker_id': task['worker_id'],
        'retry_count': task['retry_count']
    }
    logger.info(f"âœ… Task status: {task['status']} - (result_path: {task['result_path']})")

    # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œå°è¯•è¿”å›è§£æå†…å®¹
    if task['status'] == 'completed':
        if not task['result_path']:
            # ç»“æœæ–‡ä»¶å·²è¢«æ¸…ç†
            response['data'] = None
            response['message'] = 'Task completed but result files have been cleaned up (older than retention period)'
            return response

        result_dir = Path(task['result_path'])
        logger.info(f"ğŸ“‚ Checking result directory: {result_dir}")

        if result_dir.exists():
            logger.info(f"âœ… Result directory exists")
            # é€’å½’æŸ¥æ‰¾ Markdown æ–‡ä»¶ï¼ˆMinerU è¾“å‡ºç»“æ„ï¼štask_id/filename/auto/*.mdï¼‰
            md_files = list(result_dir.rglob('*.md'))
            # é€’å½’æŸ¥æ‰¾ JSON æ–‡ä»¶ (æ’é™¤è°ƒè¯•ç”¨çš„ page_*.json)
            json_files = [f for f in result_dir.rglob('*.json')
                          if not f.parent.name.startswith('page_') and f.name in ['content.json', 'result.json']]
            logger.info(f"ğŸ“„ Found {len(md_files)} markdown files and {len(json_files)} json files")

            if md_files:
                try:
                    # åˆå§‹åŒ– data å­—æ®µ
                    response['data'] = {}

                    # æ ‡è®° JSON æ˜¯å¦å¯ç”¨
                    response['data']['json_available'] = len(json_files) > 0

                    # æ ¹æ® format å‚æ•°å†³å®šè¿”å›å†…å®¹
                    if format in ['markdown', 'both']:
                        # è¯»å– Markdown å†…å®¹
                        md_file = md_files[0]
                        logger.info(f"ğŸ“– Reading markdown file: {md_file}")
                        with open(md_file, 'r', encoding='utf-8') as f:
                            md_content = f.read()

                        logger.info(f"âœ… Markdown content loaded, length: {len(md_content)} characters")

                        # æŸ¥æ‰¾å›¾ç‰‡ç›®å½•ï¼ˆåœ¨ markdown æ–‡ä»¶çš„åŒçº§ç›®å½•ä¸‹ï¼‰
                        image_dir = md_file.parent / 'images'

                        # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if upload_images and image_dir.exists():
                            logger.info(f"ğŸ–¼ï¸  Processing images for task {task_id}, upload_images={upload_images}")
                            md_content = process_markdown_images(md_content, image_dir, upload_images)

                        # æ·»åŠ  Markdown ç›¸å…³å­—æ®µ
                        response['data']['markdown_file'] = md_file.name
                        response['data']['content'] = md_content
                        response['data']['images_uploaded'] = upload_images
                        response['data']['has_images'] = image_dir.exists() if not upload_images else None

                    # å¦‚æœç”¨æˆ·è¯·æ±‚ JSON æ ¼å¼
                    if format in ['json', 'both'] and json_files:
                        import json as json_lib
                        json_file = json_files[0]
                        logger.info(f"ğŸ“– Reading JSON file: {json_file}")
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                json_content = json_lib.load(f)
                            response['data']['json_file'] = json_file.name
                            response['data']['json_content'] = json_content
                            logger.info(f"âœ… JSON content loaded successfully")
                        except Exception as json_e:
                            logger.warning(f"âš ï¸  Failed to load JSON: {json_e}")
                    elif format == 'json' and not json_files:
                        # ç”¨æˆ·è¯·æ±‚ JSON ä½†æ²¡æœ‰ JSON æ–‡ä»¶
                        logger.warning(f"âš ï¸  JSON format requested but no JSON file available")
                        response['data']['message'] = 'JSON format not available for this backend'

                    # å¦‚æœæ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ï¼Œæ·»åŠ æç¤º
                    if not response['data']:
                        response['data'] = None
                        logger.warning(f"âš ï¸  No data returned for format: {format}")
                    else:
                        logger.info(f"âœ… Response data field added successfully (format={format})")

                except Exception as e:
                    logger.error(f"âŒ Failed to read content: {e}")
                    logger.exception(e)
                    # è¯»å–å¤±è´¥ä¸å½±å“çŠ¶æ€æŸ¥è¯¢ï¼Œåªæ˜¯ä¸è¿”å› data
                    response['data'] = None
            else:
                logger.warning(f"âš ï¸  No markdown files found in {result_dir}")
        else:
            logger.error(f"âŒ Result directory does not exist: {result_dir}")
    elif task['status'] == 'completed':
        logger.warning(f"âš ï¸  Task completed but result_path is empty")
    else:
        logger.info(f"â„¹ï¸  Task status is {task['status']}, skipping content loading")

    return response


@app.delete("/api/v1/tasks/{task_id}")
async def cancel_task(task_id: str):
    """
    å–æ¶ˆä»»åŠ¡ï¼ˆä»…é™ pending çŠ¶æ€ï¼‰
    """
    task = db.get_task(task_id)

    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    if task['status'] == 'pending':
        db.update_task_status(task_id, 'cancelled')

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        file_path = Path(task['file_path'])
        if file_path.exists():
            file_path.unlink()

        logger.info(f"â¹ï¸  Task cancelled: {task_id}")
        return {
            'success': True,
            'message': 'Task cancelled successfully'
        }
    else:
        raise HTTPException(
            status_code=400,
            detail=f"Cannot cancel task in {task['status']} status"
        )


@app.get("/api/v1/queue/stats")
async def get_queue_stats():
    """
    è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
    """
    stats = db.get_queue_stats()

    return {
        'success': True,
        'stats': stats,
        'total': sum(stats.values()),
        'timestamp': datetime.now().isoformat()
    }


@app.get("/api/v1/queue/tasks")
async def list_tasks(
    status: Optional[str] = Query(None, description="ç­›é€‰çŠ¶æ€: pending/processing/completed/failed"),
    limit: int = Query(100, description="è¿”å›æ•°é‡é™åˆ¶", le=1000)
):
    """
    è·å–ä»»åŠ¡åˆ—è¡¨
    """
    if status:
        tasks = db.get_tasks_by_status(status, limit)
    else:
        # è¿”å›æ‰€æœ‰ä»»åŠ¡ï¼ˆéœ€è¦ä¿®æ”¹ TaskDB æ·»åŠ è¿™ä¸ªæ–¹æ³•ï¼‰
        with db.get_cursor() as cursor:
            cursor.execute('''
                SELECT * FROM tasks 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
            tasks = [dict(row) for row in cursor.fetchall()]

    return {
        'success': True,
        'count': len(tasks),
        'tasks': tasks
    }


@app.post("/api/v1/admin/cleanup")
async def cleanup_old_tasks(days: int = Query(7, description="æ¸…ç†Nå¤©å‰çš„ä»»åŠ¡")):
    """
    æ¸…ç†æ—§ä»»åŠ¡è®°å½•ï¼ˆç®¡ç†æ¥å£ï¼‰
    """
    deleted_count = db.cleanup_old_tasks(days)

    logger.info(f"ğŸ§¹ Cleaned up {deleted_count} old tasks")

    return {
        'success': True,
        'deleted_count': deleted_count,
        'message': f'Cleaned up tasks older than {days} days'
    }


@app.post("/api/v1/admin/reset-stale")
async def reset_stale_tasks(timeout_minutes: int = Query(60, description="è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")):
    """
    é‡ç½®è¶…æ—¶çš„ processing ä»»åŠ¡ï¼ˆç®¡ç†æ¥å£ï¼‰
    """
    reset_count = db.reset_stale_tasks(timeout_minutes)

    logger.info(f"ğŸ”„ Reset {reset_count} stale tasks")

    return {
        'success': True,
        'reset_count': reset_count,
        'message': f'Reset tasks processing for more than {timeout_minutes} minutes'
    }


@app.get("/api/v1/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        stats = db.get_queue_stats()

        return {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'database': 'connected',
            'queue_stats': stats
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return JSONResponse(
            status_code=503,
            content={
                'status': 'unhealthy',
                'error': str(e)
            }
        )


if __name__ == '__main__':
    # ä»ç¯å¢ƒå˜é‡è¯»å–ç«¯å£ï¼Œé»˜è®¤ä¸º8000
    api_port = int(os.getenv('API_PORT', '8000'))

    logger.info("ğŸš€ Starting MinerU Tianshu API Server...")
    logger.info(f"ğŸ“– API Documentation: http://localhost:{api_port}/docs")

    uvicorn.run(
        app,
        host='0.0.0.0',
        port=api_port,
        log_level='info'
    )

